{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgztVlrtkYRkZFbIKaYGJ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iftekharchowdhuryJOY/100-Days-of-Python-Code/blob/main/Final_Project_PLDA(1st_version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bishop’s University\n",
        "##Department of Computer Science\n",
        "## Name: IFTEKHARUL ISLAM CHOWDHURY\n",
        "##Intelligent systems and Neural Nets – CS504\n",
        "##FL2024 – Project"
      ],
      "metadata": {
        "id": "6qnv4Mghy9S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing the Dataset:\n",
        "\n",
        "UCI Machine Learning Repository:\n",
        "\n",
        "The dataset can be accessed directly from the UCI repository.\n",
        "Breast Cancer Wisconsin (Diagnostic) Data Set - UCI Repository"
      ],
      "metadata": {
        "id": "7rzJf6-7zVwz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c5AUqlINy5Tf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset from UCI repository\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
        "column_names = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n",
        "data = pd.read_csv(url, header=None, names=column_names)\n",
        "\n",
        "# Alternatively, load dataset from a local CSV file\n",
        "# data = pd.read_csv('path_to_downloaded_csv_file.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the Data\n",
        "1. Drop Unnecessary Columns"
      ],
      "metadata": {
        "id": "nAq0EmY3zjhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns=['ID'])"
      ],
      "metadata": {
        "id": "hFRcEaL1zl8o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Encode Target Variable"
      ],
      "metadata": {
        "id": "fxtekeNU001F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Diagnosis'] = data['Diagnosis'].map({'M': 1, 'B': 0})\n"
      ],
      "metadata": {
        "id": "4iw1bUxP06c_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Separate Features and Target"
      ],
      "metadata": {
        "id": "-VzgVwFL0-mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(columns=['Diagnosis'])\n",
        "y = data['Diagnosis']"
      ],
      "metadata": {
        "id": "XQd_Kj3g1BlQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Split the Data: Divide the dataset into training (60%), validation (20%), and test (20%) sets.**"
      ],
      "metadata": {
        "id": "upFnacU71Gpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "print(\"Training set class distribution:\", y_train.value_counts())\n",
        "print(\"Validation set class distribution:\", y_val.value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeTyxLmv1QeS",
        "outputId": "53cef587-12e7-4ef1-905b-867c02dea4c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set class distribution: Diagnosis\n",
            "0    214\n",
            "1    127\n",
            "Name: count, dtype: int64\n",
            "Validation set class distribution: Diagnosis\n",
            "0    71\n",
            "1    43\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Feature Scaling: Apply Min-Max scaling to the features.**"
      ],
      "metadata": {
        "id": "xtxKhiXd1Wxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "import numpy as np\n",
        "\n",
        "# Clip negative values to zero\n",
        "X_train_scaled = np.clip(X_train_scaled, 0, None)\n",
        "X_val_scaled = np.clip(X_val_scaled, 0, None)\n",
        "X_test_scaled = np.clip(X_test_scaled, 0, None)\n",
        "\n",
        "\n",
        "print(\"Minimum value in X_train_scaled:\", X_train_scaled.min())\n",
        "print(\"Minimum value in X_val_scaled:\", X_val_scaled.min())\n",
        "print(\"Minimum value in X_test_scaled:\", X_test_scaled.min())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3A4Vfes1cE-",
        "outputId": "ed74f803-ef02-4ad9-8dca-156891d967fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value in X_train_scaled: 0.0\n",
            "Minimum value in X_val_scaled: 0.0\n",
            "Minimum value in X_test_scaled: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Dimensionality Reduction: PCA**"
      ],
      "metadata": {
        "id": "YGO7ZXCY1fO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "n_components = 10  # Adjust based on your analysis\n",
        "pca = PCA(n_components=n_components, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_val_pca = pca.transform(X_val_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "9C6n66Dp1jnw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf = NMF(n_components=n_components, init='random', random_state=42)\n",
        "X_train_nmf = nmf.fit_transform(X_train_scaled)\n",
        "X_val_nmf = nmf.transform(X_val_scaled)\n",
        "X_test_nmf = nmf.transform(X_test_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKDS5wpj1o0p",
        "outputId": "5f90d2a4-250a-4311-9947-0a63ec79afe6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Classifiers: Train each classifier individually using cross-validation on the training set, applying default parameters for each model.**"
      ],
      "metadata": {
        "id": "tZ3wYfiKe61l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Define classifiers with default parameters\n",
        "lr = LogisticRegression(random_state=42)\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "\n",
        "# Initialize dictionary to store cross-validation results\n",
        "cv_results = {\n",
        "    'PCA_LR': None,\n",
        "    'NMF_LR': None,\n",
        "    'PCA_SVM': None,\n",
        "    'NMF_SVM': None\n",
        "}\n",
        "\n",
        "# Train Logistic Regression with PCA-transformed features\n",
        "cv_results['PCA_LR'] = cross_val_score(lr, X_train_pca, y_train, cv=5)\n",
        "\n",
        "# Train Logistic Regression with NMF-transformed features\n",
        "cv_results['NMF_LR'] = cross_val_score(lr, X_train_nmf, y_train, cv=5)\n",
        "\n",
        "# Train SVM with PCA-transformed features\n",
        "cv_results['PCA_SVM'] = cross_val_score(svm, X_train_pca, y_train, cv=5)\n",
        "\n",
        "# Train SVM with NMF-transformed features\n",
        "cv_results['NMF_SVM'] = cross_val_score(svm, X_train_nmf, y_train, cv=5)\n",
        "\n",
        "# Calculate mean cross-validation scores for each model\n",
        "mean_scores = {model: np.mean(scores) for model, scores in cv_results.items()}\n",
        "mean_scores\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bThwkcbyHWd",
        "outputId": "3d07fcb1-d5b5-4391-9623-f8f55d4f8274"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PCA_LR': 0.9619778346121057,\n",
              " 'NMF_LR': 0.9238277919863599,\n",
              " 'PCA_SVM': 0.9589087809036659,\n",
              " 'NMF_SVM': 0.9502131287297528}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict Labels: Apply the trained classifiers to the validation set to obtain predicted labels.**"
      ],
      "metadata": {
        "id": "b1ZC5_ApzA9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train Logistic Regression and SVM with PCA and NMF features\n",
        "lr_pca = LogisticRegression(C=10, random_state=42).fit(X_train_pca, y_train)\n",
        "lr_nmf = LogisticRegression(C=10, random_state=42).fit(X_train_nmf, y_train)\n",
        "\n",
        "svm_pca = SVC(C=10, kernel='rbf', random_state=42).fit(X_train_pca, y_train)\n",
        "svm_nmf = SVC(C=10, kernel='rbf', random_state=42).fit(X_train_nmf, y_train)\n",
        "\n",
        "# Predict labels on the validation set\n",
        "y_val_pred_pca_lr = lr_pca.predict(X_val_pca)\n",
        "y_val_pred_nmf_lr = lr_nmf.predict(X_val_nmf)\n",
        "y_val_pred_pca_svm = svm_pca.predict(X_val_pca)\n",
        "y_val_pred_nmf_svm = svm_nmf.predict(X_val_nmf)\n",
        "\n",
        "# Display a few predictions for each model\n",
        "print(\"Logistic Regression with PCA:\", y_val_pred_pca_lr[:5])\n",
        "print(\"Logistic Regression with NMF:\", y_val_pred_nmf_lr[:5])\n",
        "print(\"SVM with PCA:\", y_val_pred_pca_svm[:5])\n",
        "print(\"SVM with NMF:\", y_val_pred_nmf_svm[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX9ShGrOzEu5",
        "outputId": "48e27a5e-d512-499a-c3bc-dab34f4a5dc5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with PCA: [0 0 0 0 0]\n",
            "Logistic Regression with NMF: [0 0 0 0 0]\n",
            "SVM with PCA: [0 0 0 0 0]\n",
            "SVM with NMF: [0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Logistic Regression with PCA:\\n\", classification_report(y_val, y_val_pred_pca_lr))\n",
        "print(\"Logistic Regression with NMF:\\n\", classification_report(y_val, y_val_pred_nmf_lr))\n",
        "print(\"SVM with PCA:\\n\", classification_report(y_val, y_val_pred_pca_svm))\n",
        "print(\"SVM with NMF:\\n\", classification_report(y_val, y_val_pred_nmf_svm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrgtEKfX2WR1",
        "outputId": "5b2d0212-f5c4-4269-bfc0-3ebd19e6c4cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with PCA:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99        71\n",
            "           1       1.00      0.95      0.98        43\n",
            "\n",
            "    accuracy                           0.98       114\n",
            "   macro avg       0.99      0.98      0.98       114\n",
            "weighted avg       0.98      0.98      0.98       114\n",
            "\n",
            "Logistic Regression with NMF:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        71\n",
            "           1       1.00      0.91      0.95        43\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.95      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n",
            "SVM with PCA:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98        71\n",
            "           1       1.00      0.93      0.96        43\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n",
            "SVM with NMF:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        71\n",
            "           1       1.00      0.84      0.91        43\n",
            "\n",
            "    accuracy                           0.94       114\n",
            "   macro avg       0.96      0.92      0.93       114\n",
            "weighted avg       0.94      0.94      0.94       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Majority Voting: Implement majority voting to determine unified predicted labels.**"
      ],
      "metadata": {
        "id": "OtzNJ33R3IJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mode\n",
        "import numpy as np\n",
        "\n",
        "# Stack predictions from each model for majority voting\n",
        "predictions = np.vstack([\n",
        "    y_val_pred_pca_lr,\n",
        "    y_val_pred_nmf_lr,\n",
        "    y_val_pred_pca_svm,\n",
        "    y_val_pred_nmf_svm\n",
        "])\n",
        "\n",
        "# Perform majority voting (most common value along each column)\n",
        "y_val_pred_majority = mode(predictions, axis=0).mode.flatten()\n",
        "\n",
        "# Display the first few predictions as a check\n",
        "print(\"Majority Voting Predictions:\", y_val_pred_majority[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1Wk1dyL3LS4",
        "outputId": "beeea82a-cdc6-4b6e-a0ff-4582d649148e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Majority Voting Predictions: [0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Evaluate majority voting predictions on the validation set\n",
        "accuracy = accuracy_score(y_val, y_val_pred_majority)\n",
        "report = classification_report(y_val, y_val_pred_majority)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9SJ8Vrt3rsJ",
        "outputId": "4c9be4dd-8620-4ad2-f4a7-cb1f25ed17f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97        71\n",
            "           1       1.00      0.88      0.94        43\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.94      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute F-Score: Calculate the F-score using the predicted labels compared to the true labels of the validation set.**"
      ],
      "metadata": {
        "id": "rYZx9jr64EAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate the F-score for the majority voting predictions on the validation set\n",
        "f_score = f1_score(y_val, y_val_pred_majority, average='weighted')\n",
        "\n",
        "print(\"Weighted F-score:\", f_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTlY_Zjo4CbD",
        "outputId": "3dd11f50-836f-41cb-dd56-3214c7d15265"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted F-score: 0.95553257040308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Iterate with Different Components: Repeat steps 2 to 6 for varying numbers of components (2, 4, 6, 8, and 10).**"
      ],
      "metadata": {
        "id": "Om_JYzRN6BoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy.stats import mode\n",
        "import numpy as np\n",
        "\n",
        "# Component counts to iterate through\n",
        "component_counts = [2, 4, 6, 8, 10]\n",
        "results = {}\n",
        "\n",
        "for n_components in component_counts:\n",
        "    print(f\"Number of Components: {n_components}\")\n",
        "\n",
        "    # Step 2: Feature Extraction using PCA and NMF\n",
        "    pca = PCA(n_components=n_components, random_state=42)\n",
        "    nmf = NMF(n_components=n_components, init='random', random_state=42)\n",
        "\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_val_pca = pca.transform(X_val_scaled)\n",
        "    X_train_nmf = nmf.fit_transform(X_train_scaled)\n",
        "    X_val_nmf = nmf.transform(X_val_scaled)\n",
        "\n",
        "    # Step 3: Train Classifiers with Cross-Validation\n",
        "    # Define classifiers with default parameters\n",
        "    lr = LogisticRegression(random_state=42)\n",
        "    svm = SVC(random_state=42)\n",
        "\n",
        "    # Fit Logistic Regression on PCA and NMF features\n",
        "    lr_pca = lr.fit(X_train_pca, y_train)\n",
        "    lr_nmf = lr.fit(X_train_nmf, y_train)\n",
        "\n",
        "    # Fit SVM on PCA and NMF features\n",
        "    svm_pca = svm.fit(X_train_pca, y_train)\n",
        "    svm_nmf = svm.fit(X_train_nmf, y_train)\n",
        "\n",
        "    # Step 4: Predict Labels on the Validation Set\n",
        "    y_val_pred_pca_lr = lr_pca.predict(X_val_pca)\n",
        "    y_val_pred_nmf_lr = lr_nmf.predict(X_val_nmf)\n",
        "    y_val_pred_pca_svm = svm_pca.predict(X_val_pca)\n",
        "    y_val_pred_nmf_svm = svm_nmf.predict(X_val_nmf)\n",
        "\n",
        "    # Step 5: Majority Voting\n",
        "    predictions = np.vstack([\n",
        "        y_val_pred_pca_lr,\n",
        "        y_val_pred_nmf_lr,\n",
        "        y_val_pred_pca_svm,\n",
        "        y_val_pred_nmf_svm\n",
        "    ])\n",
        "    y_val_pred_majority = mode(predictions, axis=0).mode.flatten()\n",
        "\n",
        "    # Step 6: Compute F-Score\n",
        "    f_score = f1_score(y_val, y_val_pred_majority, average='weighted')\n",
        "    results[n_components] = f_score\n",
        "\n",
        "# Display the F-scores for each component count\n",
        "print(\"F-scores for different component counts:\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPMZjZzo6FkL",
        "outputId": "6ca3034e-c508-4bf5-b830-74ce6144bc4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Components: 2\n",
            "Number of Components: 4\n",
            "Number of Components: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Components: 8\n",
            "Number of Components: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-scores for different component counts: {2: 0.9007452405849058, 4: 0.819535861067334, 6: 0.7401629072681705, 8: 0.8300890092879256, 10: 0.7521781286434897}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report Best F-Score: Identify and report the optimal number of components based on the highest F-score.**"
      ],
      "metadata": {
        "id": "Z8nOTIC_i2fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the component count with the highest F-score\n",
        "optimal_components = max(results, key=results.get)\n",
        "best_f_score = results[optimal_components]\n",
        "\n",
        "# Report the optimal number of components and the highest F-score\n",
        "print(f\"Optimal Number of Components: {optimal_components}\")\n",
        "print(f\"Best F-Score: {best_f_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL5BqRXLi5uE",
        "outputId": "dc16a281-f9a6-41f1-cd2f-d21986b00da4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Number of Components: 2\n",
            "Best F-Score: 0.9007452405849058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Model Configuration: You will develop five models: model_2, model_4, model_6, model_8, and model_10, where each model corresponds to a specific number of feature components used during training. For instance, model_4 is trained with four feature components. This model is an ensemble of five different combinations of feature extraction and classifier methods, represented as follows: model_4 = {(PCA_4, LR_4), (NMF_4, LR_4), (PCA_4, SVM_4), (NMF_4, SVM_4)}.**"
      ],
      "metadata": {
        "id": "f27eTe7Pmhqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Component counts to iterate through\n",
        "component_counts = [2, 4, 6, 8, 10]\n",
        "models = {}\n",
        "\n",
        "for n_components in component_counts:\n",
        "    # Initialize the model dictionary for the current component count\n",
        "    model_name = f\"model_{n_components}\"\n",
        "    models[model_name] = {}\n",
        "\n",
        "    # Step 1: Feature Extraction\n",
        "    # Initialize PCA and NMF with the current number of components\n",
        "    pca = PCA(n_components=n_components, random_state=42)\n",
        "    nmf = NMF(n_components=n_components, init='random', random_state=42)\n",
        "\n",
        "    # Transform the training set with PCA and NMF\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_train_nmf = nmf.fit_transform(X_train_scaled)\n",
        "\n",
        "    # Step 2: Train Classifiers\n",
        "    # Logistic Regression\n",
        "    lr_pca = LogisticRegression(random_state=42).fit(X_train_pca, y_train)\n",
        "    lr_nmf = LogisticRegression(random_state=42).fit(X_train_nmf, y_train)\n",
        "\n",
        "    # SVM\n",
        "    svm_pca = SVC(random_state=42).fit(X_train_pca, y_train)\n",
        "    svm_nmf = SVC(random_state=42).fit(X_train_nmf, y_train)\n",
        "\n",
        "\n",
        "    print(\"Model_best dictionary successfully created with the best models!\")\n",
        "    # Step 3: Save Trained Models\n",
        "    # Save each model configuration in the dictionary\n",
        "    models[model_name][f\"PCA_{n_components}_LR\"] = lr_pca\n",
        "    models[model_name][f\"NMF_{n_components}_LR\"] = lr_nmf\n",
        "    models[model_name][f\"PCA_{n_components}_SVM\"] = svm_pca\n",
        "    models[model_name][f\"NMF_{n_components}_SVM\"] = svm_nmf\n",
        "\n",
        "model_best = {\n",
        "    \"PCA_LR\": lr_pca,\n",
        "    \"NMF_LR\": lr_nmf,\n",
        "    \"PCA_SVM\": svm_pca,\n",
        "    \"NMF_SVM\": svm_nmf\n",
        "}\n",
        "print(\"Model_best dictionary successfully created with the best models!\")\n",
        "# Output the dictionary keys to confirm model structure\n",
        "\n",
        "print(\"Model configuration keys:\", list(models.keys()))\n",
        "for model_name, model_dict in models.items():\n",
        "    print(f\"{model_name} contains models:\", list(model_dict.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieXxi7ZATPXV",
        "outputId": "328aab18-9f47-4edc-ab77-66dbda69af58"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_best dictionary successfully created with the best models!\n",
            "Model_best dictionary successfully created with the best models!\n",
            "Model_best dictionary successfully created with the best models!\n",
            "Model_best dictionary successfully created with the best models!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_best dictionary successfully created with the best models!\n",
            "Model_best dictionary successfully created with the best models!\n",
            "Model configuration keys: ['model_2', 'model_4', 'model_6', 'model_8', 'model_10']\n",
            "model_2 contains models: ['PCA_2_LR', 'NMF_2_LR', 'PCA_2_SVM', 'NMF_2_SVM']\n",
            "model_4 contains models: ['PCA_4_LR', 'NMF_4_LR', 'PCA_4_SVM', 'NMF_4_SVM']\n",
            "model_6 contains models: ['PCA_6_LR', 'NMF_6_LR', 'PCA_6_SVM', 'NMF_6_SVM']\n",
            "model_8 contains models: ['PCA_8_LR', 'NMF_8_LR', 'PCA_8_SVM', 'NMF_8_SVM']\n",
            "model_10 contains models: ['PCA_10_LR', 'NMF_10_LR', 'PCA_10_SVM', 'NMF_10_SVM']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clip negative values to zero after scaling\n",
        "X_test_scaled = np.clip(X_test_scaled, 0, None)\n",
        "X_test_scaled += 1e-6  # Add a very small constant to ensure all values are positive\n",
        "\n",
        "print(\"Minimum value in X_test_scaled after clipping:\", X_test_scaled.min())\n",
        "print(\"Minimum value in X_test_scaled:\", X_test_scaled.min())\n",
        "print(\"Data type of X_test_scaled:\", X_test_scaled.dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIyCCXWlhg0r",
        "outputId": "37c636e7-e3c1-4c9c-a37d-c31295dd1eec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value in X_test_scaled after clipping: 1e-06\n",
            "Minimum value in X_test_scaled: 1e-06\n",
            "Data type of X_test_scaled: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_test = NMF(n_components=2, init='random', random_state=42)\n",
        "try:\n",
        "    X_test_nmf = nmf_test.fit_transform(X_test_scaled)\n",
        "    print(\"NMF transformation successful.\")\n",
        "except ValueError as e:\n",
        "    print(\"Error during NMF transformation:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwFgzqBhiew6",
        "outputId": "0ee396f1-d373-4844-ecd0-f922a973d911"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMF transformation successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy.stats import mode\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Feature Scaling on the Test Set\n",
        "# Ensure the test set is scaled using the same Min-Max scaler applied to the training set\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Add a tiny constant to ensure all values are positive, avoiding NMF issues\n",
        "X_test_scaled = np.clip(X_test_scaled, 0, None)  # Ensure no negative values remain\n",
        "X_test_scaled += 1e-6  # Add a small constant to guarantee positivity for NMF\n",
        "\n",
        "# Step 3: Feature Extraction using PCA and NMF with 2 Components (Best Configuration)\n",
        "\n",
        "# Apply PCA\n",
        "pca_best = PCA(n_components=2, random_state=42)\n",
        "X_train_pca_best = pca_best.fit_transform(X_train_scaled)  # Train PCA on the training set\n",
        "X_test_pca = pca_best.transform(X_test_scaled)  # Apply PCA transformation to the test set\n",
        "\n",
        "# Apply NMF\n",
        "nmf_best = NMF(n_components=2, init='random', random_state=42)\n",
        "X_train_nmf_best = nmf_best.fit_transform(X_train_scaled)  # Train NMF on the training set\n",
        "X_test_nmf = nmf_best.transform(X_test_scaled)  # Apply NMF transformation to the test set\n",
        "\n",
        "# Step 4: Define and Train the Best Models Using the Training Data\n",
        "# Assuming we haven't already trained them, we train each classifier with 2 components (PCA and NMF)\n",
        "\n",
        "# Logistic Regression Models\n",
        "lr_pca_best = LogisticRegression(random_state=42).fit(X_train_pca_best, y_train)\n",
        "lr_nmf_best = LogisticRegression(random_state=42).fit(X_train_nmf_best, y_train)\n",
        "\n",
        "# SVM Models\n",
        "svm_pca_best = SVC(random_state=42).fit(X_train_pca_best, y_train)\n",
        "svm_nmf_best = SVC(random_state=42).fit(X_train_nmf_best, y_train)\n",
        "\n",
        "# Store the trained models in `model_best`\n",
        "model_best = {\n",
        "    \"PCA_LR\": lr_pca_best,\n",
        "    \"NMF_LR\": lr_nmf_best,\n",
        "    \"PCA_SVM\": svm_pca_best,\n",
        "    \"NMF_SVM\": svm_nmf_best\n",
        "}\n",
        "\n",
        "# Step 5: Make Predictions on the Test Set Using Each Model in `model_best`\n",
        "\n",
        "# Make predictions on the test set with each model\n",
        "y_test_pred_pca_lr = model_best['PCA_LR'].predict(X_test_pca)\n",
        "y_test_pred_nmf_lr = model_best['NMF_LR'].predict(X_test_nmf)\n",
        "y_test_pred_pca_svm = model_best['PCA_SVM'].predict(X_test_pca)\n",
        "y_test_pred_nmf_svm = model_best['NMF_SVM'].predict(X_test_nmf)\n",
        "\n",
        "# Step 6: Perform Majority Voting on the Test Set Predictions\n",
        "\n",
        "# Stack predictions from each model for majority voting\n",
        "predictions_test = np.vstack([\n",
        "    y_test_pred_pca_lr,\n",
        "    y_test_pred_nmf_lr,\n",
        "    y_test_pred_pca_svm,\n",
        "    y_test_pred_nmf_svm\n",
        "])\n",
        "\n",
        "# Determine the majority vote for each test sample\n",
        "y_test_pred_majority = mode(predictions_test, axis=0).mode.flatten()\n",
        "\n",
        "# Step 7: Compute the F-Score for the Majority Voting Predictions\n",
        "\n",
        "# Calculate the F-score on the test set by comparing the majority-voted predictions with the true labels\n",
        "f_score_test = f1_score(y_test, y_test_pred_majority, average='weighted')\n",
        "\n",
        "print(\"F-Score on Test Set:\", f_score_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQqISBy8gaIC",
        "outputId": "742f8268-1b30-48d1-8339-3ffe96bec390"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-Score on Test Set: 0.9372286827846255\n"
          ]
        }
      ]
    }
  ]
}